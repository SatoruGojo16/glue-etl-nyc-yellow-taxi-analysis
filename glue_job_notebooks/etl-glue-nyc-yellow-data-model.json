{
	"jobConfig": {
		"name": "etl-glue-nyc-yellow-data-model",
		"description": "",
		"role": "arn:aws:iam::023495890465:role/aws-glue-user",
		"command": "glueetl",
		"version": "5.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 2,
		"maxCapacity": 2,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 480,
		"maxConcurrentRuns": 2,
		"security": "none",
		"scriptName": "etl-glue-nyc-yellow-data-model.py",
		"scriptLocation": "s3://aws-glue-assets-023495890465-us-east-1/scripts/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [
			{
				"key": "--PROCESSED_MONTH",
				"value": "1",
				"existing": false
			},
			{
				"key": "--PROCESSED_YEAR",
				"value": "2021",
				"existing": false
			},
			{
				"key": "--SOURCE_RAW_FILE_PATH",
				"value": "s3://raw-data-bucket-5f593a/nyc-yellow-uber-data/fact/yellow_tripdata_2021-01.parquet",
				"existing": false
			}
		],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2025-08-23T18:31:18.541Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-023495890465-us-east-1/temporary/",
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-023495890465-us-east-1/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null,
		"logging": false
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "import sys\nfrom awsglue.transforms import *\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nfrom pyspark.sql.functions import * \nfrom pyspark.sql.types import * \nfrom awsglue.utils import *\nfrom awsglue.dynamicframe import DynamicFrame\nimport boto3 as b \nfrom pyspark import SparkFiles\nfrom botocore.exceptions import ClientError\nfrom datetime import datetime \n\n## @params: [JOB_NAME]\nargs = getResolvedOptions(sys.argv, ['JOB_NAME','SOURCE_RAW_FILE_PATH','PROCESSED_YEAR','PROCESSED_MONTH'])\n\n\nsc = SparkContext()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\njob.init(args['JOB_NAME'], args)\n\nglue = b.client('glue')\ndatabase_name = 'db_nyc_uber'\ndim_output_path = 's3://processed-data-bucket-5f593a/nyc-yellow-uber-data/dim/'\nfact_input_path = args['SOURCE_RAW_FILE_PATH']\nfact_ouput_path = 's3://processed-data-bucket-5f593a/nyc-yellow-uber-data/fact/'\n\nprint(fact_input_path)\n\ndef check_db_exists(database_name):\n    try:\n        response = glue.get_database(Name=database_name)\n        return True \n    except ClientError as e:\n        if e.response['Error']['Code'] == 'EntityNotFoundException':\n            return False\n        else:\n            print('Boto3 Client Error - '+str(e))\n            raise e\n    except Exception as e:\n        print('Error - ' + str(e)) \n        raise  e\n\ndef create_db_if_not_exists(database_name):\n    if not check_db_exists(database_name): \n        glue.create_database(DatabaseInput={\"Name\":database_name}) \n        print(f\"Database {database_name} is created!\")\n    else:\n        print(f\"Database {database_name} already exists!\")\n\ndef load_dataframe_to_s3(df, output_path, database_name, table_name):\n    dyf = DynamicFrame.fromDF(df, glueContext)\n    s3output = glueContext.getSink(\n        path=output_path,\n        connection_type=\"s3\",\n        updateBehavior=\"UPDATE_IN_DATABASE\",\n        compression=\"snappy\",\n        enableUpdateCatalog=True,\n    )\n    s3output.setCatalogInfo(catalogDatabase=database_name, catalogTableName=table_name)\n    s3output.setFormat(\"glueparquet\")\n    s3output.writeFrame(dyf)\n\ndef check_table_exists(database_name, table_name):\n    try:\n        glue.get_table(DatabaseName = database_name, Name=table_name)\n        return True \n    except ClientError as e:\n        if e.response['Error']['Code'] == 'EntityNotFoundException':\n            return False\n        else:\n            print('Boto3 Client Error - '+str(e))\n            raise e\n    except Exception as e:\n        print('Error - ' + str(e)) \n        raise  e\n\ndef create_table_if_not_exists(data, ouput_path, database_name, table_name, is_dataset=True):\n    if not check_table_exists(database_name, table_name):\n        if is_dataset:\n            df = spark.createDataFrame(data)\n        else:\n            df = data\n        load_dataframe_to_s3(df, ouput_path+table_name, database_name, table_name)\n        print(f\"Table {database_name+'.'+table_name} is created and loaded with data!\")\n    else:\n        print(f\"Table {database_name+'.'+table_name} already exists!\")\n\n# Creating Database `db_nyc_uber` if not exists in Glue Catalog \ncreate_db_if_not_exists(database_name)\n\n\n# Creating Dimension Tables if not exists on pre configured values from the Requirement's Data Contract \ndim_vendors = [\n    {\"vendor_id\":\"1\",\"vendor_name\":\"Creative Mobile Technologies, LLC\",},\n    {\"vendor_id\":\"2\",\"vendor_name\":\"Curb Mobility, LLC\",},\n    {\"vendor_id\":\"6\",\"vendor_name\":\"Myle Technologies Inc\",},\n    {\"vendor_id\":\"7\",\"vendor_name\":\"Helix\"}\n]\ncreate_table_if_not_exists(dim_vendors, dim_output_path, database_name, 'dim_vendors')\n\ndim_ratecode = [\n    {\"rate_code_id\":\"1\", \"rate_code_description\":\"Standard rate\"},\n    {\"rate_code_id\":\"2\", \"rate_code_description\":\"JFK\"},\n    {\"rate_code_id\":\"3\", \"rate_code_description\":\"Newark\"},\n    {\"rate_code_id\":\"4\", \"rate_code_description\":\"Nassau or Westchester\"},\n    {\"rate_code_id\":\"5\", \"rate_code_description\":\"Negotiated fare\"},\n    {\"rate_code_id\":\"6\", \"rate_code_description\":\"Group ride\"},\n    {\"rate_code_id\":\"99\", \"rate_code_description\":\"Null/unknown\"}\n]\ncreate_table_if_not_exists(dim_ratecode, dim_output_path, database_name, 'dim_ratecode')\n\ndim_store_and_fwd_flag = [{\"store_and_fwd_flag_id\":\"1\", \"store_and_fwd_flag_description\":\"store and forward trip\"},\n{\"store_and_fwd_flag_id\":\"0\", \"store_and_fwd_flag_description\":\"not a store and forward trip\"}\n]\ncreate_table_if_not_exists(dim_store_and_fwd_flag, dim_output_path, database_name, 'dim_store_and_fwd_flag')\n\ndim_payment_type = [\n    {\"payment_type_id\":\"0\", \"payment_type_description\":\"Flex Fare trip\"},\n    {\"payment_type_id\":\"1\", \"payment_type_description\":\"Credit card\"},\n    {\"payment_type_id\":\"2\", \"payment_type_description\":\"Cash\"},\n    {\"payment_type_id\":\"3\", \"payment_type_description\":\"No charge\"},\n    {\"payment_type_id\":\"4\", \"payment_type_description\":\"Dispute\"},\n    {\"payment_type_id\":\"5\", \"payment_type_description\":\"Unknown\"},\n    {\"payment_type_id\":\"6\", \"payment_type_description\":\"Voided trip\"}\n]\ncreate_table_if_not_exists(dim_payment_type, dim_output_path, database_name, 'dim_payment_type')\n\ndim_date = spark.sql('''\nSELECT explode(sequence(to_date('2000-01-01'), to_date('2030-01-01'))) as date\n''')\n\ndim_date_cols = {\n    \"date_id\":date_format(dim_date.date,'yMMdd'),\n    \"date\":date_format(dim_date.date, 'd'),\n    \"month\":date_format(dim_date.date, 'M'),\n    \"year\":date_format(dim_date.date, 'y'),\n    \"day_short\":date_format(dim_date.date, \"E\"), \n    \"day_long\":date_format(dim_date.date, \"EEEE\"), \n    \"month_short\":date_format(dim_date.date, \"LLL\"), \n    \"month_long\":date_format(dim_date.date, \"LLLL\"),\n    \"is_weekend\": when((date_format(dim_date.date, \"EEE\").isin('Sat','Sun')),'Yes').otherwise('No')\n}\ndim_date = dim_date.withColumns(dim_date_cols)\ncreate_table_if_not_exists(dim_date, dim_output_path, database_name, 'dim_date', is_dataset=False)\n\ndim_trip_peak_band = [{\n'trip_peak_band_id':'101','trip_peak_band_description':'Night Hour',\n'trip_peak_band_id':'102','trip_peak_band_description':'Peak Hour',\n'trip_peak_band_id':'103','trip_peak_band_description':'Off-Peak',\n}]\ncreate_table_if_not_exists(dim_trip_peak_band, dim_output_path, database_name, 'dim_trip_peak_band')\n\ndim_time = spark.sql('''\nSELECT explode(sequence(to_timestamp('2000-01-01 00:00:00'), to_timestamp('2000-01-01 23:59:59'), interval 1 second)) as date\n''')\ndim_time.createOrReplaceTempView('dim_time')\ndim_time = spark.sql('''\n                    select date_format(date,'HHmmss') time_id,\n                    date_format(date, 'H') hour,\n                    date_format(date, 'm') minute,\n                    date_format(date, 's') second\n                    from dim_time \n                    ''')\ncreate_table_if_not_exists(dim_time, dim_output_path, database_name, 'dim_time',  is_dataset=False)\n\ntaxi_zone_lookup_url = 's3://raw-data-bucket-5f593a/lookups/taxi_zone_lookup.csv'\ndim_taxi_zone_lookup = spark.read.csv(taxi_zone_lookup_url, header=True)\ncreate_table_if_not_exists(dim_taxi_zone_lookup, dim_output_path, database_name, 'dim_taxi_zone_lookup', is_dataset=False)\n\n# ETL - S3 File Upload Event processed via Lambda \n\ndf = spark.read.parquet(fact_input_path)\n\nrename_column_dict = {'VendorID':'vendor_id','RatecodeID':'rate_code_id','PULocationID':'pickup_location_id','DOLocationID':'drop_off_location_id'}\n\ndf = df.withColumnsRenamed(rename_column_dict)\n\ncast_column_dict ={'vendor_id': col('vendor_id').cast(IntegerType()),\n            'tpep_pickup_datetime':date_format(col('tpep_pickup_datetime'),'yyyy-MM-dd HH:mm:ss'),\n            'tpep_dropoff_datetime':date_format(col('tpep_dropoff_datetime'),'yyyy-MM-dd HH:mm:ss'),\n            'passenger_count':col('passenger_count').cast(IntegerType()),\n            'trip_distance':col('trip_distance').cast(FloatType()),\n            'rate_code_id':col('rate_code_id').cast(IntegerType()),\n            'store_and_fwd_flag':when(col('store_and_fwd_flag').cast(StringType()) == 'Y', 1).otherwise(0),\n            'pickup_location_id':col('pickup_location_id').cast(IntegerType()),\n            'drop_off_location_id':col('drop_off_location_id').cast(IntegerType()),\n            'payment_type':col('payment_type').cast(IntegerType()),\n            'fare_amount':col('fare_amount').cast(FloatType()),\n            'extra':col('extra').cast(FloatType()),\n            'mta_tax':col('mta_tax').cast(FloatType()),\n            'tip_amount':col('tip_amount').cast(FloatType()),\n            'improvement_surcharge':col('improvement_surcharge').cast(FloatType()),\n            'total_amount':col('total_amount').cast(DecimalType(10,2)),\n            'congestion_surcharge':col('congestion_surcharge').cast(FloatType()),\n            'airport_fee':coalesce(col('airport_fee').cast(FloatType()), lit(0.0)),\n                } \ndf = df.withColumns(cast_column_dict)\n\ndf = df.drop_duplicates() \n\ndf = df.filter(df.passenger_count >= 1).filter(df.passenger_count <= 6)\n\ndf = df.filter(df.trip_distance >= 5.0).filter(df.trip_distance <= 500.0)\n\ndf = df.filter('fare_amount > 0 ')\n\ndf = df.withColumn('trip_id', expr('uuid()'))\n\ndate_cols = {'tpep_pickup_date_id':date_format('tpep_pickup_datetime','yyyyMMdd'),'tpep_pickup_time_id':date_format('tpep_pickup_datetime','HHmmss'),\n            'tpep_dropoff_date_id':date_format('tpep_dropoff_datetime','yyyyMMdd'),'tpep_dropoff_time_id':date_format('tpep_dropoff_datetime','HHmmss')\n}\n\ndf = df.withColumns(date_cols)\n\ndf = df.withColumn('trip_duration_minutes',floor((unix_timestamp(df['tpep_dropoff_datetime']) - unix_timestamp(df['tpep_pickup_datetime']))/60))\n\ndf = df.filter(df['trip_duration_minutes'] < 1440)\n\ndf = df.withColumn(\"Hour\",date_format('tpep_pickup_datetime','HH'))\n\ndf_trip_peak_band = df.select(date_format('tpep_pickup_datetime','HH').alias('Hour')).distinct().select('Hour'\n            ,when(col('Hour').between(0,5) | col('Hour').between(20,23) , '101')\n            .when(col('Hour').between(6,9) | col('Hour').between(16,19) , '102')\n            .otherwise('103').alias('trip_peak_band_id'))\n\ndf = df.join(\n    df_trip_peak_band, df_trip_peak_band.Hour == df.Hour\n).select(df['*'], df_trip_peak_band.trip_peak_band_id)\n\ndf = df.drop('tpep_pickup_datetime','tpep_dropoff_datetime','Hour')\n\nPROCESSED_YEAR = args['PROCESSED_YEAR']\nPROCESSED_MONTH = args['PROCESSED_MONTH']\n\ndf = df.withColumn('processed_year', lit(PROCESSED_YEAR))\ndf = df.withColumn('processed_month', lit(PROCESSED_MONTH))\n\ndf = df.select('trip_id','vendor_id','passenger_count','tpep_pickup_date_id','tpep_pickup_time_id','tpep_dropoff_date_id','tpep_dropoff_time_id','trip_duration_minutes','trip_peak_band_id',\n               'trip_distance','rate_code_id','store_and_fwd_flag','pickup_location_id','drop_off_location_id','payment_type','fare_amount','extra','mta_tax',\n               'tip_amount','tolls_amount','improvement_surcharge','congestion_surcharge','airport_fee','total_amount',\n               'processed_year','processed_month')\n\ndyf = DynamicFrame.fromDF(df, glueContext)\ns3output = glueContext.getSink(\n    path=fact_ouput_path,\n    connection_type=\"s3\",\n    updateBehavior=\"UPDATE_IN_DATABASE\",\n    partitionKeys=['processed_year','processed_month'],\n    compression=\"snappy\",\n    enableUpdateCatalog=True,\n\n)\ns3output.setCatalogInfo(catalogDatabase=database_name, catalogTableName='fact_uber_trips')\ns3output.setFormat(\"glueparquet\")\ns3output.writeFrame(dyf)\n\n\n\njob.commit()"
}