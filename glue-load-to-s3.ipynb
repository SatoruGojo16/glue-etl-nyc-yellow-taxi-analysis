{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "289fc78f-ef55-459f-a4dd-5174e826d929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from awsglue.context import GlueContext\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.transforms import *\n",
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql.types import * \n",
    "from pyspark.sql import SparkSession\n",
    "from awsglue.utils import *\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "import sys  \n",
    "from datetime import datetime \n",
    "import pandas as pd\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c1f7dfa-2373-4103-bbb3-1f6850cfe455",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext()\n",
    "glueContext=GlueContext(sc)\n",
    "spark=glueContext.spark_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f52a504-8746-443a-a6bf-12cce3c56c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3 as b\n",
    "\n",
    "load_flag = False \n",
    "s3_client  = b.client('s3')\n",
    "response = s3_client.list_objects_v2(Bucket=\"raw-data-bucket\", Prefix=\"nyc_taxi_datasets/lookups/\")\n",
    "if 'Contents' in response:\n",
    "    load_flag = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "953d4e94-33eb-40c3-bf31-cf1d20c3c7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_vendors_data = [{\"vendor_id\":\"1\",\"vendor_name\":\"Creative Mobile Technologies, LLC\",},\n",
    "{\"vendor_id\":\"2\",\"vendor_name\":\"Curb Mobility, LLC\",},\n",
    "{\"vendor_id\":\"6\",\"vendor_name\":\"Myle Technologies Inc\",},\n",
    "{\"vendor_id\":\"7\",\"vendor_name\":\"Helix\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ebf9cff-55a6-4681-a505-6c499e122343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------------------+\n",
      "|vendor_id|vendor_name                      |\n",
      "+---------+---------------------------------+\n",
      "|1        |Creative Mobile Technologies, LLC|\n",
      "|2        |Curb Mobility, LLC               |\n",
      "|6        |Myle Technologies Inc            |\n",
      "|7        |Helix                            |\n",
      "+---------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(dim_vendors_data).show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7d54a07-f02a-4637-827b-f0597419bc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------------+\n",
      "|rate_code_description|rate_code_id|\n",
      "+---------------------+------------+\n",
      "|Standard rate        |1           |\n",
      "|JFK                  |2           |\n",
      "|Newark               |3           |\n",
      "|Nassau or Westchester|4           |\n",
      "|Negotiated fare      |5           |\n",
      "|Group ride           |6           |\n",
      "|Null/unknown         |99          |\n",
      "+---------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_ratecode_data = [{\"rate_code_id\":\"1\", \"rate_code_description\":\"Standard rate\"},\n",
    "{\"rate_code_id\":\"2\", \"rate_code_description\":\"JFK\"},\n",
    "{\"rate_code_id\":\"3\", \"rate_code_description\":\"Newark\"},\n",
    "{\"rate_code_id\":\"4\", \"rate_code_description\":\"Nassau or Westchester\"},\n",
    "{\"rate_code_id\":\"5\", \"rate_code_description\":\"Negotiated fare\"},\n",
    "{\"rate_code_id\":\"6\", \"rate_code_description\":\"Group ride\"},\n",
    "{\"rate_code_id\":\"99\", \"rate_code_description\":\"Null/unknown\"}\n",
    "]\n",
    "spark.createDataFrame(dim_ratecode_data).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acb25245-104f-4ef9-b900-6a6a41ced2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+---------------------+\n",
      "|store_and_fwd_flag_description|store_and_fwd_flag_id|\n",
      "+------------------------------+---------------------+\n",
      "|store and forward trip        |Y                    |\n",
      "|not a store and forward trip  |N                    |\n",
      "+------------------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_store_and_fwd_flag_data = [{\"store_and_fwd_flag_id\":\"Y\", \"store_and_fwd_flag_description\":\"store and forward trip\"},\n",
    "{\"store_and_fwd_flag_id\":\"N\", \"store_and_fwd_flag_description\":\"not a store and forward trip\"}\n",
    "]\n",
    "spark.createDataFrame(dim_store_and_fwd_flag_data).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b001055e-0f63-419a-8bac-886022d3cf29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+---------------+\n",
      "|payment_type_description|payment_type_id|\n",
      "+------------------------+---------------+\n",
      "|Flex Fare trip          |0              |\n",
      "|Credit card             |1              |\n",
      "|Cash                    |2              |\n",
      "|No charge               |3              |\n",
      "|Dispute                 |4              |\n",
      "|Unknown                 |5              |\n",
      "|Voided trip             |6              |\n",
      "+------------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_payment_type_data = [{\"payment_type_id\":\"0\", \"payment_type_description\":\"Flex Fare trip\"},\n",
    "{\"payment_type_id\":\"1\", \"payment_type_description\":\"Credit card\"},\n",
    "{\"payment_type_id\":\"2\", \"payment_type_description\":\"Cash\"},\n",
    "{\"payment_type_id\":\"3\", \"payment_type_description\":\"No charge\"},\n",
    "{\"payment_type_id\":\"4\", \"payment_type_description\":\"Dispute\"},\n",
    "{\"payment_type_id\":\"5\", \"payment_type_description\":\"Unknown\"},\n",
    "{\"payment_type_id\":\"6\", \"payment_type_description\":\"Voided trip\"}\n",
    "]\n",
    "dim_payment_type = spark.createDataFrame(dim_payment_type_data)\n",
    "dim_payment_type.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a588c731-3ee9-4bf9-bd01-f82bb585f330",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_dates = pd.date_range('2000-01-01','2030-01-01').to_frame(index=0, name = 'date')\n",
    "dim_date = spark.createDataFrame(pdf_dates)\n",
    "dim_date.createOrReplaceTempView('dim_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16c0315a-2ece-43bf-8bf0-c368c7c731fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_date = spark.sql('''\n",
    "SELECT explode(sequence(to_date('2000-01-01'), to_date('2030-01-01'))) as date\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dc1705f-5b1f-4461-a4b2-286831dec295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----+----+---------+---------+-----------+----------+----------+\n",
      "|date| date_id|month|year|day_short| day_long|month_short|month_long|is_weekend|\n",
      "+----+--------+-----+----+---------+---------+-----------+----------+----------+\n",
      "|   1|20000101|    1|2000|      Sat| Saturday|        Jan|   January|       Yes|\n",
      "|   2|20000102|    1|2000|      Sun|   Sunday|        Jan|   January|       Yes|\n",
      "|   3|20000103|    1|2000|      Mon|   Monday|        Jan|   January|        No|\n",
      "|   4|20000104|    1|2000|      Tue|  Tuesday|        Jan|   January|        No|\n",
      "|   5|20000105|    1|2000|      Wed|Wednesday|        Jan|   January|        No|\n",
      "|   6|20000106|    1|2000|      Thu| Thursday|        Jan|   January|        No|\n",
      "|   7|20000107|    1|2000|      Fri|   Friday|        Jan|   January|        No|\n",
      "|   8|20000108|    1|2000|      Sat| Saturday|        Jan|   January|       Yes|\n",
      "|   9|20000109|    1|2000|      Sun|   Sunday|        Jan|   January|       Yes|\n",
      "|  10|20000110|    1|2000|      Mon|   Monday|        Jan|   January|        No|\n",
      "|  11|20000111|    1|2000|      Tue|  Tuesday|        Jan|   January|        No|\n",
      "|  12|20000112|    1|2000|      Wed|Wednesday|        Jan|   January|        No|\n",
      "|  13|20000113|    1|2000|      Thu| Thursday|        Jan|   January|        No|\n",
      "|  14|20000114|    1|2000|      Fri|   Friday|        Jan|   January|        No|\n",
      "|  15|20000115|    1|2000|      Sat| Saturday|        Jan|   January|       Yes|\n",
      "|  16|20000116|    1|2000|      Sun|   Sunday|        Jan|   January|       Yes|\n",
      "|  17|20000117|    1|2000|      Mon|   Monday|        Jan|   January|        No|\n",
      "|  18|20000118|    1|2000|      Tue|  Tuesday|        Jan|   January|        No|\n",
      "|  19|20000119|    1|2000|      Wed|Wednesday|        Jan|   January|        No|\n",
      "|  20|20000120|    1|2000|      Thu| Thursday|        Jan|   January|        No|\n",
      "+----+--------+-----+----+---------+---------+-----------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_date_cols = {\n",
    "\"date_id\":date_format(dim_date.date,'yMMdd'),\n",
    "\"date\":date_format(dim_date.date, 'd'),\n",
    "\"month\":date_format(dim_date.date, 'M'),\n",
    "\"year\":date_format(dim_date.date, 'y'),\n",
    "\"day_short\":date_format(dim_date.date, \"E\"), \n",
    "\"day_long\":date_format(dim_date.date, \"EEEE\"), \n",
    "\"month_short\":date_format(dim_date.date, \"LLL\"), \n",
    "\"month_long\":date_format(dim_date.date, \"LLLL\"),\n",
    "\"is_weekend\": when((date_format(dim_date.date, \"EEE\").isin('Sat','Sun')),'Yes').otherwise('No')\n",
    "}\n",
    "dim_date = dim_date.withColumns(dim_date_cols)\n",
    "dim_date.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c929eaa5-d6a3-4046-8651-b331261c54c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_date = spark.sql('''select \n",
    "date_format(date,'yMMdd') date_id,\n",
    "date_format(date, 'd') day,\n",
    "date_format(date, 'M') month,\n",
    "date_format(date, 'y') year,\n",
    "date_format(date, \"E\") day_short, \n",
    "date_format(date, \"EEEE\") day_long, \n",
    "date_format(date, \"LLL\") month_short, \n",
    "date_format(date, \"LLLL\") month_long, \n",
    "case when date_format(date, \"EEE\") in ('Sat','Sun') then 'Yes' else 'No' end as is_weekend\n",
    "from dim_date ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9542e81d-fbb3-42e7-ae6c-f0d00b30da70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21/3279604598.py:1: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.\n",
      "  pdf_time = pd.date_range('2000-01-01 00:00:00','2000-01-01 23:59:59', freq='S', inclusive='both').to_frame(index=0, name = 'date')\n"
     ]
    }
   ],
   "source": [
    "pdf_time = pd.date_range('2000-01-01 00:00:00','2000-01-01 23:59:59', freq='S', inclusive='both').to_frame(index=0, name = 'date')\n",
    "dim_time = spark.createDataFrame(pdf_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe2e18df-d9b3-426f-946c-f719f85185ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_time = spark.sql('''\n",
    "SELECT explode(sequence(to_timestamp('2000-01-01 00:00:00'), to_timestamp('2000-01-02 23:59:59'), interval 1 second)) as date\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32c6b0d3-1494-421b-91ee-154398d97249",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_time.createOrReplaceTempView('dim_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "460178b0-e90b-42f2-90d6-230168e521df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_time = spark.sql('''select date_format(date,'HHmmss') time_id,\n",
    "date_format(date, 'H') hour,\n",
    "date_format(date, 'm') minute,\n",
    "date_format(date, 's') second\n",
    "from dim_time ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45a1c271-3097-47dc-8c73-808ab59ba583",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/07 17:52:40 WARN DAGScheduler: Broadcasting large task binary with size 1388.9 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+------+\n",
      "|time_id|hour|minute|second|\n",
      "+-------+----+------+------+\n",
      "|000000 |0   |0     |0     |\n",
      "|000001 |0   |0     |1     |\n",
      "|000002 |0   |0     |2     |\n",
      "|000003 |0   |0     |3     |\n",
      "|000004 |0   |0     |4     |\n",
      "|000005 |0   |0     |5     |\n",
      "|000006 |0   |0     |6     |\n",
      "|000007 |0   |0     |7     |\n",
      "|000008 |0   |0     |8     |\n",
      "|000009 |0   |0     |9     |\n",
      "|000010 |0   |0     |10    |\n",
      "|000011 |0   |0     |11    |\n",
      "|000012 |0   |0     |12    |\n",
      "|000013 |0   |0     |13    |\n",
      "|000014 |0   |0     |14    |\n",
      "|000015 |0   |0     |15    |\n",
      "|000016 |0   |0     |16    |\n",
      "|000017 |0   |0     |17    |\n",
      "|000018 |0   |0     |18    |\n",
      "|000019 |0   |0     |19    |\n",
      "+-------+----+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_time.show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c55009b3-1475-4273-8b09-b0bb0b10f3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41db1dd9-cc92-49eb-8fc6-5894952c838f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.addFile('https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "167b6b81-9072-4aa2-a4f6-21add784947d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = SparkFiles.get('taxi_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b422241-1e54-4351-8e94-a19ddbe895a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/spark-6c3cd8ac-b7ec-43dd-b669-a73502f89f8e/userFiles-c11af852-448a-465f-a959-112bf335e0d2/taxi_zone_lookup.csv'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c453052-fe96-4abe-b104-984d3f56a76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "|        20|        Bronx|             Belmont|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv('file:///'+path, header=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ca5c3f-d25d-41e0-95d0-3adb9dbf068f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
